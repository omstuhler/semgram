if(be_entity){
###############################################################################################
tryCatch({
being_entity = semgram:::be_7(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting characterization motifs (be_7). Some characterization motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
being_entity <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Characterization = character())
})
###############################################################################################
if(fast){
tokens$being_entity_c = NA
} else {
tryCatch({
being_entity_c = semgram:::be_8(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting characterization motifs (be_8). Some characterization motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
being_entity_c <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Characterization = character())
})
}
} else {
being_entity =
being_entity_c = data.table(doc_id = character(), ann_id = factor(), Entity = character(), Characterization = character())
}
}
###############################################################################################
########################################Posessions#############################################
###############################################################################################
if("H" %in% motif_classes){
if(verbose){cat("Extracting possessions\n")}
###############################################################################################
tryCatch({
posessive_o = semgram:::H_1(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting posession motifs (H_1). Some posession motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
posessive_o <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
tryCatch({
have_nsubj_obj_conj_act = semgram:::H_3(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting possession motifs (H_3). Some possession motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
have_nsubj_obj_conj_act <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
###############################################################################################
if(fast){
posessive_of =
have_nsubj_obj_conj =
have_nsubj_conj_obj_act =
have_nsubj_conj_subj_cons_obj =
have_xcomp_act_obj =
have_xcomp_act_obj_vconj =
have_xcomp_act_obj_nconj =
have_xcomp_act_obj_nconj_vconj = data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
} else {
tryCatch({
posessive_of = semgram:::H_2(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting posession motifs (H_2). Some posession motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
posessive_of <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
tryCatch({
have_nsubj_obj_conj = semgram:::H_4(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting possession motifs (H_4). Some possession motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
have_nsubj_obj_conj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
tryCatch({
have_nsubj_conj_obj_act = semgram:::H_5(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting possession motifs (H_5). Some possession motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
have_nsubj_conj_obj_act <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
tryCatch({
have_nsubj_conj_subj_cons_obj = semgram:::H_6(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting possession motifs (H_6). Some possession motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
have_nsubj_conj_subj_cons_obj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
tryCatch({
have_xcomp_act_obj = semgram:::H_7(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting possession motifs (H_7). Some possession motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
have_xcomp_act_obj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
tryCatch({
have_xcomp_act_obj_vconj = semgram:::H_8(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting possession motifs (H_8). Some possession motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
have_xcomp_act_obj_vconj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
tryCatch({
have_xcomp_act_obj_nconj = semgram:::H_9(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting possession motifs (H_9). Some possession motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
have_xcomp_act_obj_nconj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
tryCatch({
have_xcomp_act_obj_nconj_vconj = semgram:::H_10(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting possession motifs (H_10). Some possession motifs might not have been extracted properly. This is a secondary rule and you can probably proceed.")
have_xcomp_act_obj_nconj_vconj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), Possession = character())
})
}
}
###############################################################################################
######################################Lowercase extractions####################################
###############################################################################################
###############################################################################################
##### Lowercase
if(lowercase){
tokens$token = tolower(tokens$token)
tokens$lemma = tolower(tokens$lemma)
}
###############################################################################################
###################################Combine the motifs to a list################################
###############################################################################################
###############################################################################################
actions = if("a" %in% motif_classes){
actions = rbind(nsubj_act_conj,
nsubj_act_noun_conj_verb_conj,
by_act,
by_act_noun_conjunct,
by_act_2,
by_act_2_1,
by_act_2_noun_conj,
by_act_2_noun_conj_1,
xcomp_act_conj_verb,
xcomp_act_conj_noun)
if(lowercase){
actions$Entity = tolower(actions$Entity)
actions$action = tolower(actions$action)
}
if(nrow(actions)>0){
actions = with(actions,
{data.frame(lapply(actions[,1:3], rep, times=lengths(action)),action=unlist(action))}
)
} else {character(0)}
} else {character(0)}
###############################################################################################
treatments = if("t" %in% motif_classes){
treatments = rbind(dobj_treat,
dobj_conj_treat,
obj_of_by_act,
obj_of_by_act_nconj
)
if(lowercase){
treatments$Entity = tolower(treatments$Entity)
treatments$treatment = tolower(treatments$treatment)
}
if(nrow(treatments)>0){
treatments = with(treatments,
{data.frame(lapply(treatments[,1:3], rep, times=lengths(treatment)),treatment=unlist(treatment))}
)
} else {character(0)}
} else {character(0)}
###############################################################################################
characterizations = if("be" %in% motif_classes){
characterizations = rbind(being_adj,
being_adj_nconj,
being_adj_vconj,
being_adj_xcomp,
appos_char,
amod_adj,
being_entity,
being_entity_c
)
if(lowercase){
characterizations$Entity = tolower(characterizations$Entity)
characterizations$characterization = tolower(characterizations$characterization)
}
if(nrow(characterizations)>0){
characterizations = with(characterizations,
{data.frame(lapply(characterizations[,1:3], rep, times=lengths(characterization)),characterization=unlist(characterization))}
)
} else {character(0)}
} else {character(0)}
###############################################################################################
possessions = if("H" %in% motif_classes){
possessions = rbind(posessive_o, posessive_of,
have_nsubj_obj_conj_act,
have_nsubj_obj_conj,
have_nsubj_conj_obj_act,
have_nsubj_conj_subj_cons_obj,
have_xcomp_act_obj,
have_xcomp_act_obj_vconj,
have_xcomp_act_obj_nconj,
have_xcomp_act_obj_nconj_vconj
)
if(lowercase){
possessions$Entity = tolower(possessions$Entity)
possessions$characterization = tolower(possessions$characterization)
}
if(nrow(possessions)>0){
possessions = with(possessions,
{data.frame(lapply(possessions[,1:3], rep, times=lengths(Possession)),Possession=unlist(Possession))}
)
} else {character(0)}
} else {character(0)}
agent_treatments = rbind(dobj_treat_actor_At_casted,
dobj_treat_conj_actor_At_casted,
dobj_nconj_treat_At_casted,
by_act_agent_At_casted,
obj_of_by_act_nconj_ac_At_casted
)
agent_treatments
dobj_treat_conj_actor_At_casted
dobj_treat_actor_At_casted
possessions
motif_classes
possessions = rbind(posessive_o, posessive_of,
have_nsubj_obj_conj_act,
have_nsubj_obj_conj,
have_nsubj_conj_obj_act,
have_nsubj_conj_subj_cons_obj,
have_xcomp_act_obj,
have_xcomp_act_obj_vconj,
have_xcomp_act_obj_nconj,
have_xcomp_act_obj_nconj_vconj
)
###############################################################################################
possessions = if("H" %in% motif_classes){
possessions = rbind(posessive_o, posessive_of,
have_nsubj_obj_conj_act,
have_nsubj_obj_conj,
have_nsubj_conj_obj_act,
have_nsubj_conj_subj_cons_obj,
have_xcomp_act_obj,
have_xcomp_act_obj_vconj,
have_xcomp_act_obj_nconj,
have_xcomp_act_obj_nconj_vconj
)
if(lowercase){
possessions$Entity = tolower(possessions$Entity)
possessions$characterization = tolower(possessions$characterization)
}
if(nrow(possessions)>0){
possessions = with(possessions,
{data.frame(lapply(possessions[,1:3], rep, times=lengths(Possession)),Possession=unlist(Possession))}
)
} else {character(0)}
} else {character(0)}
possessions
being_adj
tokens = NA
entities = NA
motif_classes = c("A", "t", "a", "P", "be", "H", "At", "aP")
fast = F
parse_multi_token_entities = T
extract = "lemma"
markup = T
be_entity = T
get_aux_verbs = F
aux_verb_markup = T
pron_as_ap = F
use_appos = T
lowercase = F
verbose = F
entities = c("Emil","Christiano Ronaldo", "Donald")
text = "Emil was nice. Emil has a house. Emil's house is cool. John and Emil have a dog. Donald and Emil chased the thief. Peter chased Emil. Christiano Ronaldo scored the goal. Steve asked Fred. Emil chased the thief. Steven and Emil chased the thief. Emil then won? My friend Emil came."
tokens_df = spacy_parse(text, dependency=T, entity = F)
tokens = tokens_df
###############################################################################################
##### Text input
if(!exists("tokens")){
message("It seems you didn't provide a tokens object.\n")
}
if(!exists("entities")){
message("It seems you didn't specify any core entities to extract motifs around.\n")
}
###############################################################################################
##### Unify column labels in tokens dataframe
if("sentence_id" %in% names(tokens)){
names(tokens)[which(names(tokens) == "sentence_id")] = "sentence"
}
if("dep_rel" %in% names(tokens)){
names(tokens)[which(names(tokens) == "dep_rel")] = "relation"
}
# Get all instances of entities that are multigrams
if(T %in% str_detect(entities, " ") & parse_multi_token_entities == T){
for(entity in entities){
if(entity == entities[1]){
phrase_df = data.frame()
master_length = dim(tokens)[1]
}
entity_split = unlist(spacy_parse(entity)$token)
entity_length = length(entity_split)
if(entity_length>1){
if(verbose){cat("Replacing phrase: ", entity, "\n")}
length_seq = 1:entity_length
which_first_id = which(tokens[,"token"] == entity_split[1])
for(rownumber in which_first_id-1){
if(isTRUE(all.equal(unname(unlist(tokens[c(rownumber+length_seq), "lemma"])), entity_split))){
for(token in length_seq){
row_index = c(rownumber+token)
if(nrow(phrase_df) == 0){
phrase_df = rbind(phrase_df, c(tokens[row_index,],
phrase_replacement = paste(entity_split, collapse = " ")),
stringsAsFactors = F)
} else {
if(!paste(tokens[row_index,][,c("doc_id", "sentence","token_id")], collapse = "_") %in%
str_c(phrase_df$doc_id, phrase_df$sentence, phrase_df$token_id, sep = "_")){
phrase_df = rbind(phrase_df, c(tokens[row_index,],
phrase_replacement = paste(entity_split, collapse = " ")),
stringsAsFactors = F)
}
}
}
}
}
}
}
# Merge in the phrase replacements
if(exists("phrase_df") & nrow(phrase_df) > 0){
tokens = left_join(tokens, phrase_df[,c("doc_id", "sentence", "token_id", "phrase_replacement")],
by = c("doc_id" = "doc_id",
"sentence" = "sentence",
"token_id" = "token_id"),
keep = F)
tokens$token = ifelse(!is.na(tokens$phrase_replacement), tokens$phrase_replacement, tokens$token)
#tokens = unique(tokens)
} else {
tokens$phrase_replacement = NA
}
} else {
tokens$phrase_replacement = NA
}
if(T %in% str_detect(entities, " ") & parse_multi_token_entities == F){
message("Warning: multi-token entities were detected but parsing them was set to FALSE.\n")
}
###############################################################################################
##### Mark up tokens with eligible appos children
if(use_appos){
appos_child = tquery(token = entities, relation = "appos",
parents(pos = c("NOUN", "PROPN", "PRON"), NOT(token = entities),
label = "appos_child", fill = F
)
)
tokens = tokens %>%
annotate_tqueries("appos_child", appos_child, overwrite = T, copy = F)
tokens[,c(ncol(tokens)-1,ncol(tokens))] = NULL
} else {
tokens$appos_child = ""
}
###############################################################################################
##### Set pronoun parameter
if(pron_as_ap){
agent_patient_pos = c("NOUN", "PROPN", "PRON")
tokens$lemma = ifelse(tokens$lemma == "-PRON-", tokens$token, tokens$lemma)
} else {
agent_patient_pos = c("NOUN", "PROPN")
}
# Get going to, need to, etc. instances
if(aux_verb_markup){
if(verbose){cat("Replacing aux phrases.. ", "\n")}
aux_phrases = list(c("going", "to"),
c("need", "to"), c("needed", "to"),
c("have", "to"), c("had", "to"),
c("ought", "to"))
for(aux_phrase in 1:length(aux_phrases)){
if(aux_phrase == 1){
aux_phrase_df = data.frame()
master_length = dim(tokens)[1]
}
aux_phrase_split = aux_phrases[[aux_phrase]]
length_seq = 1:2
which_first_id = which(tokens[,"token"] == aux_phrase_split[1])
for(rownumber in which_first_id-1){
if(isTRUE(all.equal(unname(unlist(tokens[c(rownumber+length_seq), "token"])), aux_phrase_split)) &
tokens[rownumber+length_seq[2], "relation"] == "aux"){
for(token in length_seq){
row_index = c(rownumber+token)
if(nrow(aux_phrase_df) == 0){
aux_phrase_df = rbind(aux_phrase_df, c(tokens[row_index,],
aux_phrase_replacement = paste(aux_phrase_split, collapse = "-")),
stringsAsFactors = F)
} else {
if(!paste(tokens[row_index,][,c("doc_id", "sentence","token_id")], collapse = "_") %in%
str_c(aux_phrase_df$doc_id, aux_phrase_df$sentence, aux_phrase_df$token_id, sep = "_")){
aux_phrase_df = rbind(aux_phrase_df, c(tokens[row_index,],
aux_phrase_replacement = paste(aux_phrase_split, collapse = "-")),
stringsAsFactors = F)
}
}
}
}
}
}
# Merge in the phrase replacements
if(exists("aux_phrase_df") & nrow(aux_phrase_df) > 0){
tokens = left_join(tokens, aux_phrase_df[,c("doc_id", "sentence", "token_id", "aux_phrase_replacement")],
by = c("doc_id" = "doc_id",
"sentence" = "sentence",
"token_id" = "token_id"),
keep = F)
tokens$token = ifelse(!is.na(tokens$aux_phrase_replacement), tokens$aux_phrase_replacement, tokens$token)
tokens$lemma = ifelse(!is.na(tokens$aux_phrase_replacement), tokens$aux_phrase_replacement, tokens$lemma)
} else {
tokens$aux_phrase_replacement = ""
}
} else {
tokens$aux_phrase_replacement = ""
}
###############################################################################################
##### Set auxiliary markup parameter
if(get_aux_verbs){
if(!aux_verb_markup){
message("You are extracting auxiliary verbs but aux_verb_markup is set to FALSE. This may not give ideal results.\n")
}
tokens$get_aux_verbs_par = "YES"
verb_pos = c("VERB", "AUX")
} else {
tokens$get_aux_verbs_par = "NO"
verb_pos = c("VERB")
}
if("a" %in% motif_classes){
if(verbose){cat("Extracting actions\n")}
###############################################################################################
##### Run fast rules
tryCatch({
nsubj_act_conj = semgram:::a_1(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_1). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
nsubj_act_conj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
###############################################################################################
##### Run slow rules
if(fast){
nsubj_act_noun_conj_verb_conj =
by_act =
by_act_noun_conjunct =
by_act_2 =
by_act_2_1 =
by_act_2_noun_conj =
by_act_2_noun_conj_1 =
xcomp_act_conj_verb =
xcomp_act_conj_noun = data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
} else {
tryCatch({
nsubj_act_noun_conj_verb_conj = semgram:::a_2(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_2). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
nsubj_act_noun_conj_verb_conj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
tryCatch({
by_act = semgram:::a_3(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_3). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
by_act <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
tryCatch({
by_act_noun_conjunct = semgram:::a_4(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_4). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
by_act_noun_conjunct <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
tryCatch({
by_act_2 = semgram:::a_5(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_5). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
by_act_2 <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
tryCatch({
by_act_2_1 = semgram:::a_6(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_6). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
by_act_2_1 <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
tryCatch({
by_act_2_noun_conj = semgram:::a_7(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_7). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
by_act_2_noun_conj <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
tryCatch({
by_act_2_noun_conj_1 = semgram:::a_8(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_8). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
by_act_2_noun_conj_1 <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
tryCatch({
xcomp_act_conj_verb = semgram:::a_9(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_9). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
xcomp_act_conj_verb <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
tryCatch({
xcomp_act_conj_noun = semgram:::a_10(tokens, entities, verb_pos, agent_patient_pos, extract)
}, error = function(e){
message("There was an error in extracting action motifs (a_10). Some action motifs might not have been extracted properly. This is an important rule and you probably shouldn't proceed.")
xcomp_act_conj_noun <<- data.table(doc_id = character(), ann_id = factor(), Entity = character(), action = character())
})
}
}
nsubj_act_conj
nsubj_act_conj = semgram:::a_1(tokens, entities, verb_pos, agent_patient_pos, extract)
nsubj_act_conj
detach("package:semgram", unload=TRUE)
remove.packages("semgram")
devtools::install("~/Desktop/Development/semgram_with_triplet_function")
#devtools::install_github("omstuhler/semgram")
detach("package:semgram", unload=TRUE)
remove.packages("semgram")
devtools::install("~/Desktop/Development/semgram_with_triplet_function")
#devtools::install_github("omstuhler/semgram")
detach("package:semgram", unload=TRUE)
remove.packages("semgram")
devtools::install("~/Desktop/Development/semgram_with_triplet_function")
#devtools::install_github("omstuhler/semgram")
detach("package:semgram", unload=TRUE)
remove.packages("semgram")
devtools::install("~/Desktop/Development/semgram_with_triplet_function")
traceback()
devtools::install("~/Desktop/Development/semgram_with_triplet_function")
